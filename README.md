# Terra Fusion Project
**University of Illinois - Urbana/Champaign**  
**Authors:**
  1. Landon Clipp <clipp2@illinois.edu>
  2. MuQun Yang
  
The Terra Fusion project was created to fuse the MOPITT, CERES, MISR, ASTER and MODIS data into one, unified HDF5 "Basic Fusion" file set. One Basic Fusion **(BF)** granule contains data from each instrument over a certain time period. The BF granularity is defined as one Terra orbit. 

This file outlines the structure of the combined TERRA Fusion code, how to compile it on a local machine, and how to add
additional code to the program. This file may not be up to date during development.

## Installation
Although this program can be installed and run on local machines, it was developed under the assumption that it would be run on either the Blue Waters or the ROGER super computers. The following instructions detail how to install the program on either of the super computers.

#### Required Libraries
The Basic Fusion program requires the following non-standard C libraries to function:

1. <hdf.h>  -- HDF4 library
1. <mfhdf.h> -- A netCDF library that is included with installation of the HDF4 library
1. <hdf5.h> -- The HDF5 library
1. <hdf5_hl.h> -- The HDF5 lite API

As can be inferred, users of this program must have both HDF4 and HDF5 libraries installed on their machine. Instructions on installing can be found at the HDF website: https://www.hdfgroup.org/. When installing the libraries on your machine, it is highly recommended that you configure the installation to include both static and dynamic libraries.

In addition to the HDF libraries, the Basic Fusion program is also dependent on the szip and jpeg libraries. These dependencies arise within the HDF libraries themselves. Please note that both Blue Waters and ROGER provide pre-compiled, pre-installed versions of all of these libraries through the use of the "module" construct. You can see a list of available modules by invoking "module avail" in the terminal and grepping the output to see the desired HDF, szip, or jpeg modules you need.

### Blue Waters
1. Change into the desired directory and type "git clone https://github.com/TerraFusion/basicFusion"
1. cd into the basicFusion/bin directory
1. Open jobSubmit with your favorite text editor (e.g. "vim jobSubmit")
    - Change the directory of the "inputFiles" variable at the top of the page to a directory containing
        the 5 TERRA instrment directories (which themselves contain valid HDF files). The directory you give
        MUST contain all of the subdirectories: MOPITT, CERES, MODIS, MISR, and ASTER. Also, it must be an absolute
        path.
    - Change the "PROJDIR" variable to point to your basicFusion directory. Must be an absolute path.
    - Save this file and close it.
1. Open up the batchscript.pbs script
    - Change the PROJDIR to contain an absolute path to your basicFusion directory.
    - Change the EXENAME variable to be what you want the executable to be called.
    - Change the OUTFILENAME variable to be what you want the output HDF5 file to be called.
    - Change the FILELISTNAME variable to be what the generated file list (generated by generateInput.sh) will be named. This filename must be identical to the filename given by the generateInput.sh script (which will be updated later). It is recommended you simply call it "inputFiles.txt".
    - Change the STD_OUTFILENAME variable to be what you want the standard stream output text file to be named. All program streams are redirected to this file.
1. cd into the root project directory (where Makefile is located)
    - Two different makefiles have been provided. One compiles with static HDF libraries, "staticMakefile". The other compiles with dynamic HDF libraries, "dynamicMakefile". It goes without saying that if you want to use dynamic compilation, you must have your HDF libraries built with dynamic libraries enabled. Dynamic compiling may be useful for debugging purposes, but otherwise it is fine to use static compilation. cp the desired version of the makefile to "Makefile". **NOTE** As of 4/22/2017, staticMakefile is not functional. Use the dynamicMakefile. This should be fixed shortly.
    - Change the values of the "INCLUDE" and "LIB" variables to point to the proper HDF libraries. Depending on how you have compiled the HDF libraries (and also whether or not you plan on using the Blue Waters HDF modules), you will likely need to update all the variables INCLUDE1, INCLUDE2, LIB1, and LIB2. If all of your HDF4 and HDF5 library and include files are under the same directory, it is sufficient to point just the "1" variables to your libraries. Please refer to the library list above for all of the libraries required by the Basic Fusion program.
    - _Because every setup is slightly different, the Makefiles provided may not work as-is! If they don't, make sure that all compiler lines have visibility to the proper HDF4 and HDF5 include files, and that the linker has proper visibility to the HDF4 and HDF5 libraries._
    
### How to run on BW
There are multiple ways one can execute the BF program. First, **note that it is strongly recommended that you do NOT simply run the program by invoking "./basicFusion..."**. Running the program this way executes it on a login-node, and the IO-intensive nature of this program means that it will eat up shared resources on the login node, causing other BW users to experience poor responsiveness in the login nodes. Not to mention, this is a breach of the BW terms of use.

1. Using the jobSubmit script
